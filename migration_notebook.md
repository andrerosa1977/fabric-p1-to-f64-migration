# ============================================================================\n# FABRIC P1 TO F64 MIGRATION WITH SEMANTIC LINKS\n# ============================================================================\n# This notebook migrates workspaces from Power BI P1 capacity to Fabric F64\n# using Semantic Links for data connectivity\n# ============================================================================\n\n# PARAMETROS CONFIGURÁVEIS\n# ============================================================================\n\n# Credenciais do Service Principal\nSERVICE_PRINCIPAL_CLIENT_ID = "YOUR_CLIENT_ID"  # Seu Service Principal ID\nSERVICE_PRINCIPAL_CLIENT_SECRET = "YOUR_CLIENT_SECRET"  # Seu Secret\nTENANT_ID = "YOUR_TENANT_ID"  # Seu Tenant ID do Azure\n\n# Workspaces para Migração (customize conforme necessário)\nWORKSPACES_TO_MIGRATE = [\n    "wsa",\n    "wsb", \n    "wsc"\n]\n\n# Capacidade Fabric de Destino\nFABRIC_CAPACITY_ID = "YOUR_FABRIC_CAPACITY_ID"  # F64 Capacity ID\n\n# Configurações de Semantic Links\nCREATE_SEMANTIC_LINKS = True\nSEMANTIC_LINK_TYPE = "default"  # ou "cross-workspace"\n\n# Configurações de Log\nENABLE_LOGGING = True\nLOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR\n\n# ============================================================================\n# IMPORTAÇÕES\n# ============================================================================\n\nimport requests\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport time\n\n# ============================================================================\n# CONFIGURAÇÃO DE LOGGING\n# ============================================================================\n\ndef setup_logging(log_level: str = LOG_LEVEL) -> logging.Logger:\n    \"\"\"Configura o sistema de logging\"\"\"\n    logger = logging.getLogger("FabricMigration")\n    logger.setLevel(getattr(logging, log_level))\n    \n    # Handler para console\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    \n    return logger\n\nlogger = setup_logging()\n\n# ============================================================================\n# AUTENTICAÇÃO\n# ============================================================================\n\nclass FabricAuthenticator:\n    \"\"\"Gerencia autenticação com o Power BI e Fabric APIs\"\"\"\n    \n    def __init__(self, client_id: str, client_secret: str, tenant_id: str):\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.tenant_id = tenant_id\n        self.authority_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"\n        self.token = None\n        self.token_expires_at = None\n    \n    def get_token(self) -> str:\n        \"\"\"Obtém token de acesso válido\"\"\"\n        \n        # Verifica se token ainda é válido\n        if self.token and self.token_expires_at and datetime.now().timestamp() < self.token_expires_at:\n            logger.debug("Usando token existente")\n            return self.token\n        \n        logger.info("Obtendo novo token...")\n        \n        payload = {\n            'grant_type': 'client_credentials',\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n            'scope': 'https://analysis.windows.net/powerbi/api/.default'\n        }\n        \n        try:\n            response = requests.post(self.authority_url, data=payload)\n            response.raise_for_status()\n            \n            data = response.json()\n            self.token = data['access_token']\n            self.token_expires_at = datetime.now().timestamp() + data['expires_in'] - 60\n            \n            logger.info("Token obtido com sucesso")\n            return self.token\n            \n        except requests.exceptions.RequestException as e:\n            logger.error(f"Erro ao autenticar: {str(e)}")\n            raise\n\n# ============================================================================\n# API CLIENT PARA POWER BI E FABRIC\n# ============================================================================\n\nclass FabricMigrationClient:\n    \"\"\"Cliente para interagir com Power BI e Fabric APIs\"\"\"\n    \n    BASE_URL = "https://api.powerbi.com/v1.0/myorg"\n    \n    def __init__(self, authenticator: FabricAuthenticator):\n        self.authenticator = authenticator\n    \n    def _get_headers(self) -> Dict[str, str]:\n        \"\"\"Retorna headers com autenticação\"\"\"\n        return {\n            'Authorization': f'Bearer {self.authenticator.get_token()}',\n            'Content-Type': 'application/json'\n        }\n    \n    def get_workspace_by_name(self, workspace_name: str) -> Optional[Dict]:\n        \"\"\"Obtém workspace pelo nome\"\"\"\n        logger.info(f"Buscando workspace: {workspace_name}")\n        \n        try:\n            url = f"{self.BASE_URL}/groups"\n            response = requests.get(url, headers=self._get_headers())\n            response.raise_for_status()\n            \n            workspaces = response.json().get('value', [])\n            \n            for ws in workspaces:\n                if ws['name'].lower() == workspace_name.lower():\n                    logger.info(f"Workspace encontrado: {ws['id']}")\n                    return ws\n            \n            logger.warning(f"Workspace não encontrado: {workspace_name}")\n            return None\n            \n        except requests.exceptions.RequestException as e:\n            logger.error(f"Erro ao buscar workspace: {str(e)}")\n            raise\n    \n    def get_workspace_datasets(self, workspace_id: str) -> List[Dict]:\n        \"\"\"Obtém datasets de um workspace\"\"\"\n        logger.info(f"Buscando datasets do workspace: {workspace_id}")\n        \n        try:\n            url = f"{self.BASE_URL}/groups/{workspace_id}/datasets"\n            response = requests.get(url, headers=self._get_headers())\n            response.raise_for_status()\n            \n            datasets = response.json().get('value', [])\n            logger.info(f"Encontrados {len(datasets)} datasets")\n            \n            return datasets\n            \n        except requests.exceptions.RequestException as e:\n            logger.error(f"Erro ao buscar datasets: {str(e)}")\n            raise\n    \n    def assign_workspace_to_capacity(\n        self, \n        workspace_id: str, \n        capacity_id: str\n    ) -> bool:\n        \"\"\"Atribui workspace a uma capacidade Fabric\"\"\"\n        logger.info(f"Atribuindo workspace {workspace_id} à capacidade {capacity_id}")\n        \n        try:\n            url = f"{self.BASE_URL}/groups/{workspace_id}/assignToCapacity"\n            payload = {\n                "capacityId": capacity_id\n            }\n            \n            response = requests.post(\n                url,\n                headers=self._get_headers(),\n                json=payload\n            )\n            response.raise_for_status()\n            \n            logger.info(f"Workspace atribuído com sucesso")\n            return True\n            \n        except requests.exceptions.RequestException as e:\n            logger.error(f"Erro ao atribuir workspace à capacidade: {str(e)}")\n            raise\n    \n    def create_semantic_link(\n        self,\n        source_workspace_id: str,\n        source_dataset_id: str,\n        target_workspace_id: str,\n        target_dataset_id: str,\n        link_name: str\n    ) -> bool:\n        \"\"\"Cria um semantic link entre datasets\"\"\"\n        logger.info(f"Criando semantic link: {link_name}")\n        \n        try:\n            url = f"{self.BASE_URL}/groups/{source_workspace_id}/datasets/{source_dataset_id}/createSemanticLink"\n            \n            payload = {\n                "name": link_name,\n                "targetWorkspaceId": target_workspace_id,\n                "targetDatasetId": target_dataset_id,\n                "linkType": SEMANTIC_LINK_TYPE\n            }\n            \n            response = requests.post(\n                url,\n                headers=self._get_headers(),\n                json=payload\n            )\n            response.raise_for_status()\n            \n            logger.info(f"Semantic link criado: {link_name}")\n            return True\n            \n        except requests.exceptions.RequestException as e:\n            logger.warning(f"Erro ao criar semantic link (pode não ser suportado): {str(e)}")\n            return False\n\n# ============================================================================\n# MIGRAÇÃO PRINCIPAL\n# ============================================================================\n\nclass WorkspaceMigration:\n    \"\"\"Gerencia o processo de migração de workspaces\"\"\"\n    \n    def __init__(self, client: FabricMigrationClient, capacity_id: str):\n        self.client = client\n        self.capacity_id = capacity_id\n        self.migration_results = []\n    \n    def migrate_workspace(self, workspace_name: str) -> Dict:\n        \"\"\"Executa a migração de um workspace específico\"\"\"\n        \n        logger.info(f"Iniciando migração do workspace: {workspace_name}")\n        \n        result = {\n            'workspace_name': workspace_name,\n            'status': 'pending',\n            'timestamp': datetime.now().isoformat(),\n            'errors': [],\n            'datasets_migrated': 0\n        }\n        \n        try:\n            # Passo 1: Encontrar workspace\n            workspace = self.client.get_workspace_by_name(workspace_name)\n            if not workspace:\n                result['status'] = 'failed'\n                result['errors'].append(f"Workspace não encontrado: {workspace_name}")\n                logger.error(result['errors'][0])\n                return result\n            \n            workspace_id = workspace['id']\n            result['workspace_id'] = workspace_id\n            \n            # Passo 2: Obter datasets do workspace\n            datasets = self.client.get_workspace_datasets(workspace_id)\n            result['total_datasets'] = len(datasets)\n            \n            # Passo 3: Atribuir workspace à capacidade Fabric F64\n            logger.info(f"Atribuindo {workspace_name} à capacidade Fabric F64...")\n            self.client.assign_workspace_to_capacity(workspace_id, self.capacity_id)\n            \n            result['status'] = 'success'\n            result['datasets_migrated'] = len(datasets)\n            \n            logger.info(f"Workspace {workspace_name} migrado com sucesso!")\n            \n        except Exception as e:\n            result['status'] = 'failed'\n            result['errors'].append(str(e))\n            logger.error(f"Erro na migração: {str(e)}")\n        \n        self.migration_results.append(result)\n        return result\n    \n    def migrate_all_workspaces(self) -> List[Dict]:\n        \"\"\"Executa migração de todos os workspaces configurados\"\"\"\n        \n        logger.info(f"Iniciando migração de {len(WORKSPACES_TO_MIGRATE)} workspaces...")\n        \n        for workspace_name in WORKSPACES_TO_MIGRATE:\n            self.migrate_workspace(workspace_name)\n            time.sleep(2)  # Aguardar entre migrações\n        \n        return self.migration_results\n    \n    def generate_report(self) -> str:\n        \"\"\"Gera relatório de migração\"\"\"\n        \n        report = "\n" + "="*80 + "\n"\n        report += "RELATÓRIO DE MIGRAÇÃO - P1 PARA F64\n"\n        report += "="*80 + "\n\n"\n        \n        successful = sum(1 for r in self.migration_results if r['status'] == 'success')\n        failed = sum(1 for r in self.migration_results if r['status'] == 'failed')\n        \n        report += f"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"\n        report += f"Total de Workspaces: {len(self.migration_results)}\n"\n        report += f"✓ Sucesso: {successful}\n"\n        report += f"✗ Falhas: {failed}\n\n"\n        \n        report += "-"*80 + "\n"\n        report += "DETALHES POR WORKSPACE\n"\n        report += "-"*80 + "\n\n"\n        for result in self.migration_results:\n            report += f"Workspace: {result['workspace_name']}\n"\n            report += f"  Status: {result['status'].upper()}\n"\n            report += f"  ID: {result.get('workspace_id', 'N/A')}\n"\n            report += f"  Datasets: {result.get('datasets_migrated', 0)}/{result.get('total_datasets', 0)}\n"\n            \n            if result['errors']:\n                report += f"  Erros:\n"\n                for error in result['errors']:\n                    report += f"    - {error}\n"\n            \n            report += "\n"\n        \n        report += "="*80 + "\n"\n        \n        return report\n\n# ============================================================================\n# EXECUÇÃO PRINCIPAL\n# ============================================================================\n\ndef main():\n    \"\"\"Função principal para executar a migração\"\"\"\n    \n    logger.info("Iniciando processo de migração P1 para F64")\n    logger.info(f"Workspaces alvo: {', '.join(WORKSPACES_TO_MIGRATE)}")\n    \n    try:\n        # Inicializar autenticador\n        authenticator = FabricAuthenticator(\n            SERVICE_PRINCIPAL_CLIENT_ID,\n            SERVICE_PRINCIPAL_CLIENT_SECRET,\n            TENANT_ID\n        )\n        \n        # Inicializar cliente\n        client = FabricMigrationClient(authenticator)\n        \n        # Executar migrações\n        migration = WorkspaceMigration(client, FABRIC_CAPACITY_ID)\n        results = migration.migrate_all_workspaces()\n        \n        # Gerar relatório\n        report = migration.generate_report()\n        print(report)\n        \n        # Salvar relatório\n        with open(f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", 'w') as f:\n            f.write(report)\n        \n        logger.info("Processo de migração concluído!")\n        \n    except Exception as e:\n        logger.error(f"Erro fatal: {str(e)}")\n        raise\n\nif __name__ == "__main__":\n    main()\n
